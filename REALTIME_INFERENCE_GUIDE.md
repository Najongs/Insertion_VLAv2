# Real-time Inference Without VL Caching

ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œ VL ìºì‹± ì—†ì´ ì‘ë™í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ¯ í•µì‹¬ ì°¨ì´ì 

### í•™ìŠµ ì‹œ (VL Caching ì‚¬ìš©)
```python
# 1. ì‚¬ì „ì— VL featuresë¥¼ ìºì‹œë¡œ ì €ì¥
python Make_VL_cache.py

# 2. í•™ìŠµ ì‹œ ìºì‹œ ë¡œë“œ
vl_features = load_from_cache(cache_key)  # ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ
actions = model(vl_features, sensor, robot_states)
```

**ì¥ì :**
- VL encodingì„ í•œ ë²ˆë§Œ ìˆ˜í–‰ (ë¹ ë¦„)
- VLM reuseë¡œ íš¨ìœ¨ì 

**ë‹¨ì :**
- ì‹¤ì‹œê°„ ì¶”ë¡ ì— ì‚¬ìš© ë¶ˆê°€ (ìƒˆë¡œìš´ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¶ˆê°€)

---

### ì‹¤ì‹œê°„ ì¶”ë¡  (VL Caching ì—†ìŒ)
```python
# ë§¤ í”„ë ˆì„ VL encoding ìˆ˜í–‰
for frame in camera_stream:
    # 1. VL encoding (ì‹¤ì‹œê°„)
    vl_features = vl_model(frame, text)  # ë§¤ë²ˆ ìƒˆë¡œ encoding

    # 2. Sensor & Robot state encoding
    sensor_features = sensor_encoder(sensor_data)
    robot_features = robot_encoder(robot_states)

    # 3. Action prediction
    actions = action_expert(vl_features, sensor_features, robot_features)
```

**ì¥ì :**
- ì‹¤ì‹œê°„ ì¶”ë¡  ê°€ëŠ¥
- ìƒˆë¡œìš´ ì´ë¯¸ì§€ ì²˜ë¦¬ ê°€ëŠ¥

**ë‹¨ì :**
- VL encodingì´ ë³‘ëª© (ì „ì²´ì˜ 90% ì‹œê°„)
- ë¹„ë™ê¸° ì²˜ë¦¬ í•„ìˆ˜

---

## ğŸ”§ ì½”ë“œ ìˆ˜ì • ë°©ë²•

### 1. Model Forward ìˆ˜ì •

```python
# models/unified_model.py - QwenVLAUnified.forward()

def forward(self, text_inputs, image_inputs, ...):
    # âŒ ê¸°ì¡´: ìºì‹œ ì‚¬ìš©
    vl_tokens = self._encode_vision_features(
        text_inputs, image_inputs, cache_keys, use_cache=True
    )

    # âœ… ì‹¤ì‹œê°„: ìºì‹œ ë¯¸ì‚¬ìš©
    vl_tokens = self._encode_vision_features_realtime(
        text_inputs, image_inputs
    )
```

### 2. Real-time VL Encoding êµ¬í˜„

```python
def _encode_vision_features_realtime(self, text_inputs, image_inputs):
    """Encode VL features without caching (real-time inference)"""

    # Prepare messages
    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": img}
            for img in image_inputs
        ] + [{"type": "text", "text": text_inputs[0]}]
    }]

    # Process
    text = self.processor.apply_chat_template(messages, ...)
    vision_inputs, _ = process_vision_info(messages)

    inputs = self.processor(
        text=[text],
        images=vision_inputs,
        padding=True,
        return_tensors="pt"
    ).to(device='cuda', dtype=torch.bfloat16)

    # VL model forward
    with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
        outputs = self.vl_model(
            **inputs,
            output_hidden_states=True,
            use_cache=False,  # âœ… KV cacheë„ ë¹„í™œì„±í™”
            return_dict=True
        )
        vl_tokens = outputs.hidden_states[-1]  # (B, seq_len, 2048)

        # âœ… Mean pooling (í•™ìŠµ ì‹œ ìºì‹œì™€ ë™ì¼)
        vl_features = vl_tokens.mean(dim=1, keepdim=True)  # (B, 1, 2048)

    return vl_features
```

### 3. Dataset ìˆ˜ì • (ì‹¤ì‹œê°„ìš©)

```python
# vla_datasets/unified_dataset.py

class RealtimeVLADataset(UnifiedVLADataset):
    """Dataset for real-time inference without VL caching"""

    def __getitem__(self, idx):
        # âŒ VL cache ë¡œë“œí•˜ì§€ ì•ŠìŒ
        # cache_key = ...
        # vl_features = load_cache(cache_key)

        # âœ… ì´ë¯¸ì§€ ê²½ë¡œì™€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (ëª¨ë¸ì—ì„œ encoding)
        return {
            "images": image_paths,  # List[str]
            "instruction": instruction,  # str
            "sensor_data": sensor_data,
            "robot_states": robot_states,
            "actions": actions,
        }
```

---

## âš¡ ë¹„ë™ê¸° ì²˜ë¦¬ ì „ëµ

VL encodingì´ ëŠë¦¬ë¯€ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.

### VLM Reuse Pattern

```python
import threading
import queue

class AsyncVLEncoder:
    def __init__(self, vl_model, processor):
        self.vl_model = vl_model
        self.processor = processor
        self.vl_queue = queue.Queue(maxsize=3)  # VLM reuse count
        self.running = True

        # VL encoding ìŠ¤ë ˆë“œ ì‹œì‘
        self.thread = threading.Thread(target=self._encode_loop, daemon=True)
        self.thread.start()

    def _encode_loop(self):
        """Background thread for VL encoding"""
        while self.running:
            try:
                # ìƒˆ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                frame, text = camera.get_frame(), get_instruction()

                # VL encoding (ëŠë¦¼: ~300ms)
                vl_features = self._encode(frame, text)

                # Queueì— ì €ì¥ (reuseë¥¼ ìœ„í•´)
                self.vl_queue.put(vl_features)

            except Exception as e:
                print(f"VL encoding error: {e}")

    def get_features(self):
        """Get VL features from queue (fast)"""
        return self.vl_queue.get(timeout=0.5)
```

### Main Loop

```python
# Main control loop (10Hz)
async_encoder = AsyncVLEncoder(vl_model, processor)

while True:
    start = time.time()

    # 1. VL features ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°, ë¹ ë¦„)
    vl_features = async_encoder.get_features()

    # 2. Sensor & Robot encoding (ë¹ ë¦„: ~10ms)
    sensor_features = sensor_encoder(sensor_data)
    robot_features = robot_encoder(robot_states)

    # 3. Action prediction (ë¹ ë¦„: ~15ms)
    actions = action_expert(vl_features, sensor_features, robot_features)

    # 4. Execute actions
    robot.execute(actions[0])

    # 10Hz ìœ ì§€
    elapsed = time.time() - start
    time.sleep(max(0, 0.1 - elapsed))
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| ë°©ì‹ | VL Encoding | Action Prediction | ì´ ì‹œê°„ | FPS |
|------|-------------|-------------------|---------|-----|
| ë™ê¸° (ìºì‹œ ì—†ìŒ) | 300ms | 25ms | 325ms | 3.1 FPS |
| ë¹„ë™ê¸° (VLM reuse=3) | 300ms (ë°±ê·¸ë¼ìš´ë“œ) | 25ms | 25ms | **40 FPS** |

**VLM Reuse=3 ì˜ë¯¸:**
- VL encodingì„ 300msë§ˆë‹¤ 1ë²ˆ ìˆ˜í–‰
- Action predictionì€ 100ms(10Hz)ë§ˆë‹¤ ìˆ˜í–‰
- ê°™ì€ VL featuresë¥¼ 3ë²ˆ ì¬ì‚¬ìš©

---

## ğŸš€ Quick Start

### 1. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
```bash
# VL ìºì‹± ì—†ì´ ì‹¤ì‹œê°„ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬
bash benchmark_quick_test.sh
```

### 2. ì‹¤ì‹œê°„ ì¶”ë¡  í…ŒìŠ¤íŠ¸
```python
# test_realtime_inference.py
from models.unified_model import QwenVLAUnified

model = QwenVLAUnified(
    model_type='regression',
    sensor_enabled=True,
    robot_state_enabled=True,
)

# Disable cache for real-time
model.cache_enabled = False

# Test inference
vl_features = model._encode_vision_features_realtime(
    text_inputs=["Pick up the blue object"],
    image_inputs=[img1, img2, img3, img4, img5]
)

actions = model.action_expert(vl_features, sensor, robot)
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹¤ì‹œê°„ ì¶”ë¡ ì„ ìœ„í•œ í™•ì¸ ì‚¬í•­:

- [ ] `model.cache_enabled = False` ì„¤ì •
- [ ] VL encodingì—ì„œ `use_cache=False` ì‚¬ìš©
- [ ] Mean pooling ì ìš© (í•™ìŠµ ì‹œ ìºì‹œì™€ ë™ì¼)
- [ ] ë¹„ë™ê¸° VL encoding êµ¬í˜„
- [ ] VLM reuse pattern ì ìš©
- [ ] 10Hz ì œì–´ ì£¼ê¸° ë‹¬ì„± í™•ì¸

---

**ì‘ì„±ì¼:** 2025-01-04
**ë²„ì „:** v1.0 - Real-time Inference without Caching

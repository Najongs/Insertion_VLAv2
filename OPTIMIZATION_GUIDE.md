# VLA Training Optimization Guide

í•™ìŠµ ì†ë„ í–¥ìƒì„ ìœ„í•´ ì ìš©ëœ ìµœì í™” ì‚¬í•­ê³¼ ì‚¬ìš© ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

## ğŸ“Š ì ìš©ëœ ìµœì í™” í•­ëª©

### 1. ë°ì´í„°ì…‹ ì´ˆê¸°í™” ìµœì í™”

**ë³€ê²½ ì‚¬í•­:**
- ì—í”¼ì†Œë“œ ê²½ë¡œ ì‚¬ì „ ìˆ˜ì§‘ í›„ ì¼ê´„ ë¡œë”©
- ì§„í–‰ ìƒí™© í‘œì‹œ (tqdm í”„ë¡œê·¸ë ˆìŠ¤ ë°”)
- ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì¶œë ¥ ìµœì†Œí™”
- NPZ íŒŒì¼ ë¡œë”© ì‹œ mmap_mode ì‚¬ìš©

**íš¨ê³¼:**
- ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì‹œê°„ **30-50% ë‹¨ì¶•**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ

**ìœ„ì¹˜:**
- `TRAIN_Unified.py`: 964-1019 ë¼ì¸
- `vla_datasets/unified_dataset.py`: 151-153, 206-236, 283-325 ë¼ì¸

---

### 2. ë°ì´í„° ë¡œë”© íŒŒì´í”„ë¼ì¸ ìµœì í™”

**ë³€ê²½ ì‚¬í•­:**
```python
# Before
prefetch_factor=4
pin_memory=True

# After
prefetch_factor=6  # âœ… Increased from 4 to 6
pin_memory=True
pin_memory_device='cuda'  # âœ… Direct CUDA pinning
```

**íš¨ê³¼:**
- GPU ëŒ€ê¸° ì‹œê°„ ê°ì†Œ
- ë°ì´í„° ë¡œë”© ë³‘ëª© í˜„ìƒ ì™„í™”
- í•™ìŠµ throughput **10-15% í–¥ìƒ**

**ìœ„ì¹˜:**
- `vla_datasets/unified_dataset.py`: 841-857 ë¼ì¸

---

### 3. í•™ìŠµ ë£¨í”„ ìµœì í™”

#### 3.1 cuDNN Benchmark í™œì„±í™”
```python
# Before
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

# After
torch.backends.cudnn.benchmark = True   # âœ… 20-30% speedup
torch.backends.cudnn.deterministic = False
```

**íš¨ê³¼:**
- ê³ ì •ëœ ì…ë ¥ í¬ê¸°ì—ì„œ **20-30% ì†ë„ í–¥ìƒ**
- ì²« ëª‡ iterationì—ì„œ ìµœì  ì•Œê³ ë¦¬ì¦˜ ìë™ ì„ íƒ

**ì£¼ì˜:**
- ì¬í˜„ì„±ì´ í•„ìš”í•œ ê²½ìš° Falseë¡œ ë˜ëŒë ¤ì•¼ í•¨
- ì…ë ¥ í¬ê¸°ê°€ ë™ì ì¸ ê²½ìš° ì˜¤íˆë ¤ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ

**ìœ„ì¹˜:**
- `TRAIN_Unified.py`: 68-72 ë¼ì¸

#### 3.2 Mixed Precision Training (BFloat16)
```python
# Using BFloat16 with autocast
with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
    loss = model(...)
    loss.backward()
```

**íš¨ê³¼:**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ **25-40% ê°ì†Œ**
- í•™ìŠµ ì†ë„ **15-25% í–¥ìƒ**
- BFloat16ì€ FP16ë³´ë‹¤ ë„“ì€ dynamic rangeë¥¼ ê°€ì ¸ overflowì— ê°•í•¨

**ì£¼ì˜:**
- BFloat16ì—ì„œëŠ” GradScaler ë¶ˆí•„ìš” (FP16ì—ì„œë§Œ í•„ìš”)
- ì´ë¯¸ `torch.autocast(dtype=torch.bfloat16)` ì‚¬ìš© ì¤‘

**ìœ„ì¹˜:**
- `TRAIN_Unified.py`: 585-587, 665, 706-723 ë¼ì¸

---

### 4. CSV â†’ NPZ ìë™ ë³€í™˜ ìœ í‹¸ë¦¬í‹°

**ì‚¬ìš©ë²•:**

```bash
# ë‹¨ì¼ ì—í”¼ì†Œë“œ ë³€í™˜
python utils/convert_csv_to_npz.py --dir /path/to/episode_dir

# ì „ì²´ ë°ì´í„°ì…‹ ë³€í™˜
python utils/convert_csv_to_npz.py --dataset /home/najo/NAS/VLA/dataset/New_dataset

# Dry run (ì‹¤ì œ ë³€í™˜ ì—†ì´ í™•ì¸ë§Œ)
python utils/convert_csv_to_npz.py --dataset /home/najo/NAS/VLA/dataset/New_dataset --dry-run
```

**íš¨ê³¼:**
- robot_states ë¡œë”© ì†ë„ **10-100ë°° í–¥ìƒ**
- íŒŒì¼ í¬ê¸° **50-80% ê°ì†Œ** (ì••ì¶•)

**ê¶Œì¥ ì‚¬í•­:**
í•™ìŠµ ì‹œì‘ ì „ì— ëª¨ë“  CSV íŒŒì¼ì„ NPZë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ **ê°•ë ¥íˆ ê¶Œì¥**í•©ë‹ˆë‹¤.

---

## ğŸš€ ì„±ëŠ¥ í–¥ìƒ ìš”ì•½

| ìµœì í™” í•­ëª© | ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ |
|------------|--------------|
| ë°ì´í„°ì…‹ ì´ˆê¸°í™” | 30-50% ë¹ ë¦„ |
| ë°ì´í„° ë¡œë”© | 10-15% ë¹ ë¦„ |
| cuDNN Benchmark | 20-30% ë¹ ë¦„ |
| Mixed Precision | 15-25% ë¹ ë¦„ |
| CSV â†’ NPZ | 10-100ë°° ë¹ ë¦„ |
| **ì „ì²´ í•™ìŠµ throughput** | **40-60% í–¥ìƒ** |

---

## ğŸ“ ì‚¬ìš© ê¶Œì¥ ì‚¬í•­

### 1. CSV â†’ NPZ ë³€í™˜ (í•„ìˆ˜)
í•™ìŠµ ì‹œì‘ ì „ ë°˜ë“œì‹œ ì‹¤í–‰:
```bash
python utils/convert_csv_to_npz.py --dataset /home/najo/NAS/VLA/dataset/New_dataset
```

### 2. num_workers ì¡°ì •
ì‹œìŠ¤í…œ ì‚¬ì–‘ì— ë”°ë¼ ì¡°ì •:
```bash
# CPU ì½”ì–´ê°€ ë§ì€ ê²½ìš° (ê¶Œì¥: 4-8)
--num_workers 8

# CPU ì½”ì–´ê°€ ì ì€ ê²½ìš°
--num_workers 4
```

### 3. batch_size ì¡°ì •
GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •:
```bash
# A100 (80GB): batch_size=32
# A100 (40GB): batch_size=24
# RTX 4090 (24GB): batch_size=16
# RTX 3090 (24GB): batch_size=12
```

### 4. gradient accumulation ì¡°ì •
Effective batch sizeë¥¼ ìœ ì§€í•˜ë©´ì„œ ë©”ëª¨ë¦¬ ì ˆì•½:
```bash
# Effective batch size = batch_size Ã— grad_accum Ã— num_gpus
# ì˜ˆ: 32 Ã— 4 Ã— 4 = 512

--batch_size 32 --grad_accum 4
```

---

## âš™ï¸ ì¶”ê°€ ìµœì í™” ì˜µì…˜

### 1. torch.compile (PyTorch 2.0+, ì„ íƒì )

ëª¨ë¸ ì»´íŒŒì¼ë¡œ ì¶”ê°€ ì†ë„ í–¥ìƒ (10-20%):

```python
# In TRAIN_Unified.py, after model initialization:
model = torch.compile(model, mode='max-autotune')
```

**ì£¼ì˜:**
- ì²« iterationì´ ë§¤ìš° ëŠë¦¼ (ì»´íŒŒì¼ ì‹œê°„)
- ì¼ë¶€ ëª¨ë¸ì—ì„œ í˜¸í™˜ì„± ì´ìŠˆ ë°œìƒ ê°€ëŠ¥

### 2. Gradient Checkpointing (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)

ë©”ëª¨ë¦¬ ì ˆì•½ (ì†ë„ëŠ” ì•½ê°„ ëŠë ¤ì§):

```python
# In models/unified_model.py
model.gradient_checkpointing_enable()
```

---

## ğŸ” ëª¨ë‹ˆí„°ë§

### WandBë¥¼ í†µí•œ ì„±ëŠ¥ í™•ì¸

í•™ìŠµ ì¤‘ ë‹¤ìŒ ë©”íŠ¸ë¦­ì„ í™•ì¸:
- `train/step` - ì´ˆë‹¹ ì²˜ë¦¬ step ìˆ˜
- `system/gpu_mem_GB` - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- `system/cpu_mem_%` - CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- `train/grad_norm` - Gradient norm (ì•ˆì •ì„± í™•ì¸)

### ë²¤ì¹˜ë§ˆí¬ ë¹„êµ

ìµœì í™” ì „í›„ ë¹„êµ:
```bash
# ìµœì í™” ì „
# - ë°ì´í„°ì…‹ ë¡œë”©: ~300ì´ˆ
# - Stepë‹¹ ì‹œê°„: ~1.5ì´ˆ
# - Epochë‹¹ ì‹œê°„: ~45ë¶„

# ìµœì í™” í›„ (ì˜ˆìƒ)
# - ë°ì´í„°ì…‹ ë¡œë”©: ~150ì´ˆ (50% ë‹¨ì¶•)
# - Stepë‹¹ ì‹œê°„: ~0.9ì´ˆ (40% ë‹¨ì¶•)
# - Epochë‹¹ ì‹œê°„: ~27ë¶„ (40% ë‹¨ì¶•)
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì¬í˜„ì„± (Reproducibility)**
   - `cudnn.benchmark=True`ëŠ” ì¬í˜„ì„±ì„ ë³´ì¥í•˜ì§€ ì•ŠìŒ
   - ì •í™•í•œ ì¬í˜„ì´ í•„ìš”í•œ ê²½ìš° `cudnn.benchmark=False`ë¡œ ë˜ëŒë ¤ì•¼ í•¨

2. **ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)**
   - Mixed precisionì„ ì‚¬ìš©í•´ë„ OOM ë°œìƒ ì‹œ:
     - batch_size ê°ì†Œ
     - gradient accumulation ì¦ê°€
     - num_workers ê°ì†Œ

3. **ë°ì´í„° ë¬´ê²°ì„±**
   - CSV â†’ NPZ ë³€í™˜ í›„ ì›ë³¸ CSV íŒŒì¼ì€ ë°±ì—… ê¶Œì¥
   - ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¡œê·¸ í™•ì¸

---

## ğŸ“ ë¬¸ì œ í•´ê²°

### Q: í•™ìŠµ ì†ë„ê°€ ì—¬ì „íˆ ëŠë¦¼
A:
1. GPU ì‚¬ìš©ë¥  í™•ì¸ (`nvidia-smi`)
2. num_workers ì¡°ì •
3. prefetch_factor ì¦ê°€ ì‹œë„
4. SSD/NVMe ì €ì¥ì†Œ ì‚¬ìš© ê¶Œì¥

### Q: OOM ì—ëŸ¬ ë°œìƒ
A:
1. batch_size ê°ì†Œ
2. gradient_checkpointing í™œì„±í™”
3. ì´ë¯¸ì§€ í•´ìƒë„ ë‚®ì¶¤ (--image_resize_*)

### Q: CSV â†’ NPZ ë³€í™˜ ì‹¤íŒ¨
A:
1. íŒŒì¼ ê¶Œí•œ í™•ì¸
2. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
3. CSV íŒŒì¼ í˜•ì‹ í™•ì¸ (ì»¬ëŸ¼ëª… ì¼ì¹˜ ì—¬ë¶€)

---

## ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„

ì¶”ê°€ë¡œ ê³ ë ¤í•  ìµœì í™”:
1. **torch.compile** ì ìš© (PyTorch 2.0+)
2. **FSDP (Fully Sharded Data Parallel)** ì ìš© (8+ GPUs)
3. **FlashAttention-2** ì—…ê·¸ë ˆì´ë“œ
4. **DeepSpeed** í†µí•©

---

**ì‘ì„±ì¼:** 2025-01-04
**ë²„ì „:** v2.0 (Optimized)

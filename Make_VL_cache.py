import os
import sys
import math

from pathlib import Path
import hashlib, fcntl

from tqdm import tqdm

import torch
import torch.distributed as dist

from qwen_vl_utils import process_vision_info
from torch.utils.data import DataLoader, DistributedSampler

# Add project root to Python path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from vla_datasets.Total_Dataset import collate_fn

# =====================================
# 1Ô∏è‚É£ Action Expert (Temporal Decoder)
# =====================================
def build_vl_cache_distributed_optimized(
    model,
    dataset,
    device="cuda",
    *,
    batch_size=16,          # DataLoader Î∞∞Ïπò (VRAM 24GBÎ©¥ 2~4 Í∂åÏû•)
    num_workers=8,
    prefetch_factor=4,
    micro_bs=1,            # ÎßàÏù¥ÌÅ¨Î°ú Î∞∞Ïπò (OOM Ïãú ÏûêÎèô Î∞±Ïò§ÌîÑ)
    key_mode="full",       # "traj": traj_path Îã®ÏúÑ Ï∫êÏãú / "full": (traj+lang+views) Îã®ÏúÑ
    rank_sharded_cache=False,  # rankÎ≥Ñ Ï∫êÏãú Ìè¥Îçî Î∂ÑÎ¶¨
    cache_dir_fallback="/home/najo/NAS/VLA/dataset/cache/qwen_vl_features",
):
    """
    Í≥†ÏÜç/ÏïàÏ†ï Ï∫êÏã±:
      - ÎßàÏù¥ÌÅ¨Î°úÎ∞∞Ïπ≠ + OOM Î∞±Ïò§ÌîÑ
      - use_cache=False (KV cache ÎπÑÌôúÏÑ±Ìôî)
      - rankÎ≥Ñ Ï∫êÏãú Ìè¥Îçî Î∂ÑÎ¶¨Î°ú fcntl ÎùΩ Í≤ΩÌï© ÏµúÏÜåÌôî
      - tqdm ÏßÑÌñâÎ•†, miss/skipped ÌÜµÍ≥Ñ ÌëúÏãú
      - key_mode="traj"Î°ú ÎëêÎ©¥ ÎèôÏùº trajÎäî 1ÌöåÎßå Í≥ÑÏÇ∞ (ÏÜçÎèÑ 10~30Î∞∞‚Üë)

    model ÏöîÍµ¨ÏÇ¨Ìï≠:
      - model.vl_model, model.processor ÌïÑÏöî
      - (ÏÑ†ÌÉù) model.cache_dir ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ cache_dir_fallback ÏÇ¨Ïö©
      - (ÏÑ†ÌÉù) model._atomic_save / model._enforce_cache_limit ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
    """

    rank = dist.get_rank()
    world_size = dist.get_world_size()

    # base cache dir
    base_cache_dir = getattr(model, "cache_dir", None)
    if base_cache_dir is None:
        base_cache_dir = Path(cache_dir_fallback)
    else:
        base_cache_dir = Path(base_cache_dir)

    # rank-sharded dir (ÎùΩ Í≤ΩÏüÅ ÏµúÏÜåÌôî)
    target_cache_dir = base_cache_dir / (f"rank_{rank}" if rank_sharded_cache else "")
    target_cache_dir.mkdir(parents=True, exist_ok=True)

    # ---------------------------
    # Î°úÏª¨ Ìó¨Ìçº: ÏïàÏ†Ñ Ï†ÄÏû•/Í≤ΩÎ°ú ÏÉùÏÑ±
    # ---------------------------
    def _local_atomic_save(tensor_cpu: torch.Tensor, path: Path):
        """modelÏóê _atomic_saveÍ∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ Î°úÏª¨ Íµ¨ÌòÑ"""
        if hasattr(model, "_atomic_save"):
            return model._atomic_save(tensor_cpu, path)
        tmp = path.with_suffix(path.suffix + ".tmp")
        lock_path = str(path) + ".lock"
        with open(lock_path, "w") as lockfile:
            try:
                fcntl.flock(lockfile, fcntl.LOCK_EX)
                if path.exists():
                    return
                torch.save(tensor_cpu, tmp)
                os.replace(tmp, path)
            finally:
                fcntl.flock(lockfile, fcntl.LOCK_UN)

    def _local_enforce_cache_limit(max_gb=40):
        """modelÏóê _enforce_cache_limit ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ Î°úÏª¨Î°ú target_cache_dirÎßå Ï†ïÎ¶¨"""
        if hasattr(model, "_enforce_cache_limit"):
            try:
                # modelÏù¥ ÏûêÍ∏∞ cache_dirÏùÑ Í¥ÄÎ¶¨ÌïúÎã§Î©¥ Ìò∏Ï∂ú
                model._enforce_cache_limit(max_gb=max_gb)
                return
            except TypeError:
                pass  # ÏãúÍ∑∏ÎãàÏ≤ò Ï∞®Ïù¥ Î¨¥ÏãúÌïòÍ≥† Î°úÏª¨ Ï≤òÎ¶¨Î°ú Ìè¥Î∞±
        total_bytes = 0
        files = []
        for f in target_cache_dir.glob("*.pt"):
            total_bytes += f.stat().st_size
            files.append(f)
        limit = max_gb * (1024 ** 3)
        if total_bytes > limit:
            files = sorted(files, key=lambda f: f.stat().st_mtime)
            while total_bytes > limit and files:
                f = files.pop(0)
                total_bytes -= f.stat().st_size
                f.unlink(missing_ok=True)

    def _cache_path_for(traj_key: str, txt: str, views: list[str | None]) -> Path:
        """key_modeÏóê Îî∞Îùº Ï∫êÏãú ÌÇ§ Íµ¨ÏÑ±."""
        if key_mode == "traj":
            # traj_pathÎßå Í∏∞Ï§Ä (Í∞ôÏùÄ trajÎäî 1ÌöåÎßå Í≥ÑÏÇ∞)
            base = traj_key.split("::t=")[0]
        else:
            # Í∏∞Ï°¥ Î∞©Ïãù: traj+lang+views Î™®Îëê Î∞òÏòÅ
            vlist = [v for v in views if v is not None]
            base = traj_key + "||" + txt + "||" + "|".join(vlist)
        h = hashlib.sha1(base.encode("utf-8")).hexdigest()[:24]
        return target_cache_dir / f"{h}.pt"

    # ---------------------------
    # DataLoader (ÏÉòÌîå Î∂ÑÎ∞∞ Î≥¥Ïû•)
    # ---------------------------
    sampler = DistributedSampler(
        dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False
    )
    data_loader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        sampler=sampler,
        collate_fn=collate_fn,
        prefetch_factor=prefetch_factor if num_workers > 0 else None,
        pin_memory=False,
        persistent_workers=False,
    )

    total_local = math.ceil(len(dataset) / world_size)
    print(f"[Rank {rank}] Assigned ~{total_local} samples for caching.")
    print(f"[Rank {rank}] CUDA ready: {torch.cuda.is_available()}, device={torch.cuda.current_device()}")

    # ---------------------------
    # Ï∫êÏã± Î£®ÌîÑ
    # ---------------------------
    if hasattr(model, "eval"):
        model.eval()

    total_cached, total_skipped, total_processed = 0, 0, 0
    pbar = tqdm(
        total=total_local,
        desc=f"[Rank {rank}] Caching progress",
        dynamic_ncols=True,
        disable=(rank != 0)
    )

    with torch.inference_mode():
        for batch_idx, batch in enumerate(data_loader):
            texts = batch["instruction"]
            image_paths_list = batch["images"]
            keys = batch["cache_keys"]

            # --- ÎØ∏Ïä§/Ïä§ÌÇµ Î∂ÑÎ¶¨ ---
            miss_items = []
            for key, txt, views in zip(keys, texts, image_paths_list):
                cpath = _cache_path_for(key, txt, views)
                if not cpath.exists():
                    miss_items.append({"text": txt, "views": views, "key": key, "cpath": cpath})
                else:
                    total_skipped += 1

            total_processed += len(keys)
            if not miss_items:
                pbar.update(len(keys))
                if rank == 0:
                    cached_ratio = (total_cached / max(1, total_processed)) * 100
                    pbar.set_postfix({
                        "cached": total_cached,
                        "skipped": total_skipped,
                        "miss%": f"{100 - cached_ratio:.1f}%",
                        "GPU": f"{torch.cuda.memory_allocated(device)/1e9:.1f}GB"
                    })
                continue

            # --- Î©îÏãúÏßÄ Ï†ÑÏ≤òÎ¶¨ (CPU) ---
            messages_list = []
            for item in miss_items:
                txt, views = item["text"], item["views"]
                msg_content = [{"type": "image", "image": v} for v in views if v is not None]
                msg_content.append({"type": "text", "text": txt})
                messages_list.append([{"role": "user", "content": msg_content}])

            processed_texts, vision_inputs_list = [], []
            for messages in messages_list:
                text = model.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
                vision_inputs, _ = process_vision_info(messages)
                processed_texts.append(text)
                vision_inputs_list.append(vision_inputs)

            # --- ÎßàÏù¥ÌÅ¨Î°úÎ∞∞Ïπ≠ + OOM Î∞±Ïò§ÌîÑ ---
            start = 0
            _micro_bs = max(1, micro_bs)
            while start < len(miss_items):
                end = min(start + _micro_bs, len(miss_items))
                sub_items  = miss_items[start:end]
                sub_texts  = processed_texts[start:end]
                sub_vision = vision_inputs_list[start:end]

                try:
                    inputs = model.processor(
                        text=sub_texts,
                        images=sub_vision,
                        padding=True,
                        return_tensors="pt"
                    ).to(device=device, dtype=torch.bfloat16, non_blocking=True)

                    outputs = model.vl_model(
                        **inputs,
                        output_hidden_states=True,
                        use_cache=False,          # ‚úÖ Î©îÎ™®Î¶¨ Ï†àÍ∞ê
                        return_dict=True
                    )
                    vl_tokens_batch = outputs.hidden_states[-1]
                    pooled_batch = vl_tokens_batch.mean(dim=1, keepdim=True)

                    for j, item in enumerate(sub_items):
                        pooled_single = pooled_batch[j:j+1]
                        _local_atomic_save(
                            pooled_single.detach().to("cpu", dtype=torch.float16),
                            item["cpath"]
                        )
                        total_cached += 1

                    # Ï†ïÎ¶¨
                    del inputs, outputs, vl_tokens_batch, pooled_batch
                    torch.cuda.empty_cache()

                    start = end  # Îã§Ïùå ÎßàÏù¥ÌÅ¨Î°ú Î∞∞ÏπòÎ°ú ÏßÑÌñâ

                except RuntimeError as e:
                    if "CUDA out of memory" in str(e):
                        torch.cuda.empty_cache()
                        if _micro_bs == 1:
                            raise  # Îçî Ï§ÑÏùº Ïàò ÏóÜÏùå
                        _micro_bs = max(1, _micro_bs // 2)
                        if rank == 0:
                            print(f"[OOM] Lowering micro_bs to #{_micro_bs} and retrying...")
                        continue
                    else:
                        raise

            # --- ÏßÑÌñâÎ•† ÏóÖÎç∞Ïù¥Ìä∏ ---
            pbar.update(len(keys))
            if rank == 0:
                cached_ratio = (total_cached / max(1, total_processed)) * 100
                pbar.set_postfix({
                    "cached": total_cached,
                    "skipped": total_skipped,
                    "miss%": f"{100 - cached_ratio:.1f}%",
                    "GPU": f"{torch.cuda.memory_allocated(device)/1e9:.1f}GB"
                })

            # (ÏÑ†ÌÉù) Ï£ºÍ∏∞Ï†Å Ï∫êÏãú Ïö©Îüâ Ï†úÌïú
            _local_enforce_cache_limit(max_gb=40)

    pbar.close()
    print(f"[Rank {rank}] ‚úÖ Finished. Cached {total_cached} / Skipped {total_skipped}")
    dist.barrier()
    if rank == 0:
        print("üöÄ All ranks finished caching. Cache is ready for training.")

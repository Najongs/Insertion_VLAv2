import os
import sys
import math

from pathlib import Path
import hashlib, fcntl

from tqdm import tqdm

import torch
import torch.distributed as dist

from qwen_vl_utils import process_vision_info
from torch.utils.data import DataLoader, DistributedSampler

# Add project root to Python path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from vla_datasets.unified_dataset import unified_collate_fn
from vla_cache_manager import get_cache_manager

# =====================================
# 1Ô∏è‚É£ Action Expert (Temporal Decoder)
# =====================================
def build_vl_cache_distributed_optimized(
    model,
    dataset,
    device="cuda",
    *,
    batch_size=16,          # DataLoader Î∞∞Ïπò (VRAM 24GBÎ©¥ 2~4 Í∂åÏû•)
    num_workers=8,
    prefetch_factor=4,
    micro_bs=1,            # ÎßàÏù¥ÌÅ¨Î°ú Î∞∞Ïπò (OOM Ïãú ÏûêÎèô Î∞±Ïò§ÌîÑ)
    cache_dir_fallback="/home/najo/NAS/VLA/dataset/cache/qwen_vl_features",
):
    """
    ÏôÑÏ†Ñ Í≥†Ï†ï Ï∫êÏã± ÏãúÏä§ÌÖú (VLACacheManager ÏÇ¨Ïö©):
      - ÎßàÏù¥ÌÅ¨Î°úÎ∞∞Ïπ≠ + OOM Î∞±Ïò§ÌîÑ
      - use_cache=False (KV cache ÎπÑÌôúÏÑ±Ìôî)
      - Ï∫êÏãú Í≤ΩÎ°ú: {dataset_name}_vlm{vlm_idx}.pt (instruction/image Î≥ÄÍ≤ΩÏóê ÏòÅÌñ• ÏóÜÏùå)
      - Atomic save + Ï∫êÏãú Ïö©Îüâ Ï†úÌïú ÏûêÎèô Í¥ÄÎ¶¨
      - tqdm ÏßÑÌñâÎ•†, miss/skipped ÌÜµÍ≥Ñ ÌëúÏãú

    model ÏöîÍµ¨ÏÇ¨Ìï≠:
      - model.vl_model, model.processor ÌïÑÏöî
      - (ÏÑ†ÌÉù) model.cache_dir ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ cache_dir_fallback ÏÇ¨Ïö©
    """

    distributed = dist.is_available() and dist.is_initialized()
    if distributed:
        rank = dist.get_rank()
        world_size = dist.get_world_size()
    else:
        rank = 0
        world_size = 1

    # base cache dir
    base_cache_dir = getattr(model, "cache_dir", None)
    if base_cache_dir is None:
        base_cache_dir = Path(cache_dir_fallback)
    else:
        base_cache_dir = Path(base_cache_dir)

    # VLACacheManager Ï¥àÍ∏∞Ìôî
    cache_mgr = get_cache_manager(
        cache_dir=str(base_cache_dir),
        cache_limit_gb=50.0
    )

    # ---------------------------
    # DataLoader (ÏÉòÌîå Î∂ÑÎ∞∞ Î≥¥Ïû•)
    # ---------------------------
    sampler = None
    if distributed:
        sampler = DistributedSampler(
            dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False
        )

    data_loader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        sampler=sampler,
        shuffle=False if sampler else False,
        collate_fn=unified_collate_fn,
        prefetch_factor=prefetch_factor if num_workers > 0 else None,
        pin_memory=False,
        persistent_workers=False,
    )

    total_local = math.ceil(len(dataset) / world_size)
    print(f"[Rank {rank}] Assigned ~{total_local} samples for caching.")
    current_device = torch.cuda.current_device() if torch.cuda.is_available() else "cpu"
    print(f"[Rank {rank}] CUDA ready: {torch.cuda.is_available()}, device={current_device}")

    # ---------------------------
    # Ï∫êÏã± Î£®ÌîÑ
    # ---------------------------
    if hasattr(model, "eval"):
        model.eval()

    total_cached, total_skipped, total_processed = 0, 0, 0
    pbar = tqdm(
        total=total_local,
        desc=f"[Rank {rank}] Caching progress",
        dynamic_ncols=True,
        disable=(rank != 0)
    )

    with torch.inference_mode():
        for batch_idx, batch in enumerate(data_loader):
            texts = batch["instruction"]
            image_paths_list = batch["images"]
            cache_keys = batch["cache_keys"]
            vlm_indices = batch["vlm_indices"]

            # --- ÎØ∏Ïä§/Ïä§ÌÇµ Î∂ÑÎ¶¨ (VLACacheManager ÏÇ¨Ïö©) ---
            miss_items = []
            for cache_key, vlm_idx, txt, views in zip(cache_keys, vlm_indices, texts, image_paths_list):
                # cache_key format: "{dataset_name}_vlm{vlm_idx}"
                # Extract dataset_name
                dataset_name = cache_key.rsplit("_vlm", 1)[0]

                if not cache_mgr.cache_exists(dataset_name, vlm_idx):
                    miss_items.append({
                        "text": txt,
                        "views": views,
                        "dataset_name": dataset_name,
                        "vlm_idx": vlm_idx
                    })
                else:
                    total_skipped += 1

            total_processed += len(cache_keys)
            if not miss_items:
                pbar.update(len(cache_keys))
                if rank == 0:
                    cached_ratio = (total_cached / max(1, total_processed)) * 100
                    pbar.set_postfix({
                        "cached": total_cached,
                        "skipped": total_skipped,
                        "miss%": f"{100 - cached_ratio:.1f}%",
                        "GPU": f"{torch.cuda.memory_allocated(device)/1e9:.1f}GB"
                    })
                continue

            # --- Î©îÏãúÏßÄ Ï†ÑÏ≤òÎ¶¨ (CPU) ---
            messages_list = []
            for item in miss_items:
                txt, views = item["text"], item["views"]
                msg_content = [{"type": "image", "image": v} for v in views if v is not None]
                msg_content.append({"type": "text", "text": txt})
                messages_list.append([{"role": "user", "content": msg_content}])

            processed_texts, vision_inputs_list = [], []
            for messages in messages_list:
                text = model.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
                vision_inputs, _ = process_vision_info(messages)
                processed_texts.append(text)
                vision_inputs_list.append(vision_inputs)

            # --- ÎßàÏù¥ÌÅ¨Î°úÎ∞∞Ïπ≠ + OOM Î∞±Ïò§ÌîÑ ---
            start = 0
            _micro_bs = max(1, micro_bs)
            while start < len(miss_items):
                end = min(start + _micro_bs, len(miss_items))
                sub_items  = miss_items[start:end]
                sub_texts  = processed_texts[start:end]
                sub_vision = vision_inputs_list[start:end]

                try:
                    inputs = model.processor(
                        text=sub_texts,
                        images=sub_vision,
                        padding=True,
                        return_tensors="pt"
                    ).to(device=device, dtype=torch.bfloat16, non_blocking=True)

                    outputs = model.vl_model(
                        **inputs,
                        output_hidden_states=True,
                        use_cache=False,          # ‚úÖ Î©îÎ™®Î¶¨ Ï†àÍ∞ê
                        return_dict=True
                    )
                    vl_tokens_batch = outputs.hidden_states[-1]
                    pooled_batch = vl_tokens_batch.mean(dim=1, keepdim=True)

                    for j, item in enumerate(sub_items):
                        pooled_single = pooled_batch[j:j+1]
                        # VLACacheManagerÎ°ú Ï†ÄÏû•
                        cache_mgr.save_cache(
                            dataset_name=item["dataset_name"],
                            vlm_idx=item["vlm_idx"],
                            vl_features=pooled_single
                        )
                        total_cached += 1

                    # Ï†ïÎ¶¨
                    del inputs, outputs, vl_tokens_batch, pooled_batch
                    torch.cuda.empty_cache()

                    start = end  # Îã§Ïùå ÎßàÏù¥ÌÅ¨Î°ú Î∞∞ÏπòÎ°ú ÏßÑÌñâ

                except RuntimeError as e:
                    if "CUDA out of memory" in str(e):
                        torch.cuda.empty_cache()
                        if _micro_bs == 1:
                            raise  # Îçî Ï§ÑÏùº Ïàò ÏóÜÏùå
                        _micro_bs = max(1, _micro_bs // 2)
                        if rank == 0:
                            print(f"[OOM] Lowering micro_bs to #{_micro_bs} and retrying...")
                        continue
                    else:
                        raise

            # --- ÏßÑÌñâÎ•† ÏóÖÎç∞Ïù¥Ìä∏ ---
            pbar.update(len(cache_keys))
            if rank == 0:
                cached_ratio = (total_cached / max(1, total_processed)) * 100
                pbar.set_postfix({
                    "cached": total_cached,
                    "skipped": total_skipped,
                    "miss%": f"{100 - cached_ratio:.1f}%",
                    "GPU": f"{torch.cuda.memory_allocated(device)/1e9:.1f}GB"
                })

            # Note: Cache limit is automatically enforced by VLACacheManager.save_cache()

    pbar.close()
    print(f"[Rank {rank}] ‚úÖ Finished. Cached {total_cached} / Skipped {total_skipped}")
    dist.barrier()
    if rank == 0:
        print("üöÄ All ranks finished caching. Cache is ready for training.")

# VLA Cache Migration Guide

## ğŸ“Š í˜„ì¬ ìƒíƒœ

### ê¸°ì¡´ ìºì‹œ ì‹œìŠ¤í…œ
- **íŒŒì¼ ìˆ˜**: 1,085,457ê°œ
- **ì´ ìš©ëŸ‰**: 8.4GB
- **íŒŒì¼ëª… í˜•ì‹**: Hash ê¸°ë°˜ (ì˜ˆ: `00002a20a20e3399f3c7d146.pt`)
- **ë¬¸ì œì **: Instructionì´ë‚˜ image pathê°€ ë°”ë€Œë©´ ìºì‹œë¥¼ ëª» ì°¾ìŒ

### ìƒˆë¡œìš´ ìºì‹œ ì‹œìŠ¤í…œ
- **íŒŒì¼ëª… í˜•ì‹**: `{dataset_name}_vlm{vlm_idx}.pt`
- **ì˜ˆì‹œ**:
  - `recv_all_20251027_170308_vlm0.pt`
  - `episode_20251030_025119_vlm150.pt`
- **ì¥ì **: ì™„ì „íˆ ê²°ì •ë¡ ì , instruction/image path ë³€ê²½ì— ê°•ê±´

---

## ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜µì…˜

### Option 1: ìºì‹œ ì¬ìƒì„± (ê¶Œì¥)

**ì¥ì **:
- âœ… ê¹¨ë—í•œ ì‹œì‘
- âœ… ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ì™„ì „ í™œìš©
- âœ… ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½ (ì¤‘ë³µ ì œê±°)

**ë‹¨ì **:
- â±ï¸ ì‹œê°„ ì†Œìš” (GPU ì‚¬ìš©)

**ì ˆì°¨**:
```bash
# 1. ê¸°ì¡´ ìºì‹œ ë°±ì—… (ì„ íƒì‚¬í•­)
mv /home/najo/NAS/VLA/dataset/cache/qwen_vl_features \
   /home/najo/NAS/VLA/dataset/cache/qwen_vl_features_old_hash_backup

# 2. ìƒˆ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p /home/najo/NAS/VLA/dataset/cache/qwen_vl_features

# 3. ìƒˆë¡œìš´ ìºì‹œ ìƒì„± (Make_VL_cache.py ì‚¬ìš©)
# ë°©ë²•ì€ ì•„ë˜ "ìºì‹œ ì¬ìƒì„± ë°©ë²•" ì°¸ì¡°
```

---

### Option 2: ë‘ ì‹œìŠ¤í…œ ë³‘í–‰ (ì„ì‹œ)

**ë°©ë²•**: ìƒˆë¡œìš´ ìºì‹œë¥¼ ë³„ë„ ë””ë ‰í† ë¦¬ì— ìƒì„±
```bash
# ìƒˆ ìºì‹œë¥¼ ë‹¤ë¥¸ ìœ„ì¹˜ì— ìƒì„±
mkdir -p /home/najo/NAS/VLA/dataset/cache/qwen_vl_features_new

# TRAIN_Unified.pyì—ì„œ cache_dir ë³€ê²½
# ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
```

**ì¥ì **:
- âœ… ê¸°ì¡´ ìºì‹œ ë³´ì¡´
- âœ… ì ì§„ì  ì „í™˜ ê°€ëŠ¥

**ë‹¨ì **:
- âŒ ë””ìŠ¤í¬ ê³µê°„ 2ë°° ì‚¬ìš©
- âŒ ê´€ë¦¬ ë³µì¡

---

### Option 3: ê¸°ì¡´ ìºì‹œ ì‚­ì œ í›„ ì¬ìƒì„± (ê°„ë‹¨)

**ë°©ë²•**:
```bash
# ê²½ê³ : ê¸°ì¡´ ìºì‹œë¥¼ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤!
rm -rf /home/najo/NAS/VLA/dataset/cache/qwen_vl_features/*

# ìºì‹œ ì¬ìƒì„±
# ì•„ë˜ "ìºì‹œ ì¬ìƒì„± ë°©ë²•" ì°¸ì¡°
```

**ì¥ì **:
- âœ… ê°€ì¥ ê°„ë‹¨
- âœ… ë””ìŠ¤í¬ ê³µê°„ ì¦‰ì‹œ í™•ë³´

**ë‹¨ì **:
- âŒ ê¸°ì¡´ ìºì‹œ ì™„ì „ ì†ì‹¤
- â±ï¸ ì „ì²´ ì¬ìƒì„± í•„ìš”

---

## ğŸš€ ìºì‹œ ì¬ìƒì„± ë°©ë²•

### ì¤€ë¹„ì‚¬í•­
1. GPUê°€ ìˆëŠ” í™˜ê²½
2. `Make_VL_cache.py` ìŠ¤í¬ë¦½íŠ¸
3. í•™ìŠµ ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸

### ë‹¨ì¼ GPUë¡œ ìºì‹œ ìƒì„±
```python
# make_cache_single_gpu.py
import torch
import torch.distributed as dist
from pathlib import Path
from models.unified_model import QwenVLAUnified
from vla_datasets.unified_dataset import UnifiedVLADataset, create_unified_dataloader
from Make_VL_cache import build_vl_cache_distributed_optimized

# Initialize distributed (single process)
dist.init_process_group(backend='nccl', init_method='tcp://127.0.0.1:29500', world_size=1, rank=0)

# Load model
model = QwenVLAUnified(
    model_type='regression',
    vl_model_name="Qwen/Qwen2.5-VL-3B-Instruct",
).cuda()
model.eval()

# Create dataset (example for new format)
dataset = UnifiedVLADataset(
    data_dir="/home/najo/NAS/VLA/dataset/New_dataset/Yellow_point/episode_20251030_025119",
    format='new',
    horizon=8,
    vlm_reuse_count=3,
)

# Build cache
build_vl_cache_distributed_optimized(
    model=model,
    dataset=dataset,
    device="cuda",
    batch_size=4,  # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
    num_workers=4,
    micro_bs=1,
)

dist.destroy_process_group()
print("âœ… Cache generation complete!")
```

ì‹¤í–‰:
```bash
python make_cache_single_gpu.py
```

---

### Multi-GPUë¡œ ìºì‹œ ìƒì„± (ë” ë¹ ë¦„)

```bash
# 4 GPUs ì‚¬ìš© ì˜ˆì‹œ
torchrun --nproc_per_node=4 make_cache_multi_gpu.py
```

`make_cache_multi_gpu.py`:
```python
import torch
import torch.distributed as dist
from pathlib import Path
from models.unified_model import QwenVLAUnified
from vla_datasets.unified_dataset import UnifiedVLADataset
from Make_VL_cache import build_vl_cache_distributed_optimized

# Initialize distributed
dist.init_process_group(backend='nccl')
rank = dist.get_rank()
device = torch.device(f"cuda:{rank}")

# Load model
model = QwenVLAUnified(
    model_type='regression',
    vl_model_name="Qwen/Qwen2.5-VL-3B-Instruct",
).to(device)
model.eval()

# Create dataset
dataset = UnifiedVLADataset(
    data_dir="/home/najo/NAS/VLA/dataset/New_dataset/Yellow_point/episode_20251030_025119",
    format='new',
    horizon=8,
    vlm_reuse_count=3,
)

# Build cache (distributed)
build_vl_cache_distributed_optimized(
    model=model,
    dataset=dataset,
    device=device,
    batch_size=4,
    num_workers=4,
    micro_bs=1,
)

dist.destroy_process_group()
if rank == 0:
    print("âœ… All ranks finished. Cache generation complete!")
```

---

### ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´ ìºì‹œ ìƒì„±

```python
# make_all_caches.py
import torch
import torch.distributed as dist
from pathlib import Path
from models.unified_model import QwenVLAUnified
from vla_datasets.unified_dataset import UnifiedVLADataset
from Make_VL_cache import build_vl_cache_distributed_optimized

dist.init_process_group(backend='nccl', init_method='tcp://127.0.0.1:29500', world_size=1, rank=0)

model = QwenVLAUnified(
    model_type='regression',
    vl_model_name="Qwen/Qwen2.5-VL-3B-Instruct",
).cuda()
model.eval()

# Old format datasets
old_dataset_root = Path("/home/najo/NAS/VLA/dataset/dataset")
for traj_dir in sorted(old_dataset_root.glob("*")):
    if not traj_dir.is_dir():
        continue

    print(f"\n{'='*80}")
    print(f"Processing OLD format: {traj_dir.name}")
    print(f"{'='*80}")

    try:
        dataset = UnifiedVLADataset(
            data_dir=str(traj_dir),
            format='old',
            horizon=8,
            vlm_reuse_count=3,
        )

        build_vl_cache_distributed_optimized(
            model=model,
            dataset=dataset,
            device="cuda",
            batch_size=4,
            num_workers=4,
        )
    except Exception as e:
        print(f"âš ï¸ Failed: {e}")
        continue

# New format datasets
new_dataset_root = Path("/home/najo/NAS/VLA/dataset/New_dataset")
for color_dir in new_dataset_root.glob("*"):
    if not color_dir.is_dir():
        continue

    for episode_dir in sorted(color_dir.glob("episode_*")):
        print(f"\n{'='*80}")
        print(f"Processing NEW format: {episode_dir.name}")
        print(f"{'='*80}")

        try:
            dataset = UnifiedVLADataset(
                data_dir=str(episode_dir),
                format='new',
                horizon=8,
                vlm_reuse_count=3,
            )

            build_vl_cache_distributed_optimized(
                model=model,
                dataset=dataset,
                device="cuda",
                batch_size=4,
                num_workers=4,
            )
        except Exception as e:
            print(f"âš ï¸ Failed: {e}")
            continue

dist.destroy_process_group()
print("\nâœ… All datasets cached!")
```

ì‹¤í–‰:
```bash
# Single GPU
python make_all_caches.py

# Multi-GPU (ë” ë¹ ë¦„)
torchrun --nproc_per_node=4 make_all_caches.py
```

---

## ğŸ“Š ì˜ˆìƒ ì†Œìš” ì‹œê°„

### ë‹¨ì¼ ë°ì´í„°ì…‹ (episode 1ê°œ)
- **ìƒ˜í”Œ ìˆ˜**: ~200
- **VLM í˜¸ì¶œ**: ~67íšŒ (vlm_reuse_count=3)
- **GPU**: RTX 3090 ê¸°ì¤€
- **ì˜ˆìƒ ì‹œê°„**: 2-5ë¶„

### ì „ì²´ ë°ì´í„°ì…‹
- **ë°ì´í„°ì…‹ ìˆ˜**: ìˆ˜ì‹­~ìˆ˜ë°± ê°œ
- **Single GPU**: ìˆ˜ ì‹œê°„ ~ í•˜ë£¨
- **4 GPUs**: 1/4 ì‹œê°„

---

## âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 1. ë°±ì—… (ì„ íƒ)
- [ ] ê¸°ì¡´ ìºì‹œ ë””ë ‰í† ë¦¬ ë°±ì—…
```bash
mv /home/najo/NAS/VLA/dataset/cache/qwen_vl_features \
   /home/najo/NAS/VLA/dataset/cache/qwen_vl_features_backup
```

### 2. ìƒˆ ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„
- [ ] ë””ë ‰í† ë¦¬ ìƒì„±
```bash
mkdir -p /home/najo/NAS/VLA/dataset/cache/qwen_vl_features
```

### 3. ìºì‹œ ìƒì„±
- [ ] í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸
- [ ] ì „ì²´ ë°ì´í„°ì…‹ ìºì‹œ ìƒì„± ì‹¤í–‰

### 4. ê²€ì¦
- [ ] ìºì‹œ íŒŒì¼ í™•ì¸
```bash
ls /home/najo/NAS/VLA/dataset/cache/qwen_vl_features/ | head -20
```
- [ ] íŒŒì¼ëª… í˜•ì‹ í™•ì¸ (dataset_name_vlmN.pt)
- [ ] Dataset ë¡œë”© í…ŒìŠ¤íŠ¸
```bash
python test_cache_system.py
```

### 5. í•™ìŠµ ì‹œì‘
- [ ] TRAIN_Unified.py ì‹¤í–‰
- [ ] "VL Cache: N/N" ë¡œê·¸ í™•ì¸ (100% ì ì¤‘ í™•ì¸)

---

## ğŸ” Troubleshooting

### ë¬¸ì œ: OOM during cache generation

**í•´ê²°**:
1. `batch_size` ì¤„ì´ê¸° (4 â†’ 2 â†’ 1)
2. `micro_bs` ì¤„ì´ê¸° (ìë™ ë°±ì˜¤í”„ ìˆìŒ)
3. ëª¨ë¸ precision ë‚®ì¶”ê¸° (bfloat16 ì‚¬ìš© ì¤‘)

### ë¬¸ì œ: ìºì‹œ ìƒì„±ì´ ë„ˆë¬´ ëŠë¦¼

**í•´ê²°**:
1. Multi-GPU ì‚¬ìš©
2. `num_workers` ì¦ê°€
3. `prefetch_factor` ì¦ê°€

### ë¬¸ì œ: ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±

**í•´ê²°**:
1. ê¸°ì¡´ hash ê¸°ë°˜ ìºì‹œ ì‚­ì œ
```bash
rm /home/najo/NAS/VLA/dataset/cache/qwen_vl_features/*[0-9a-f]*.pt
```
2. Cache limit ì¡°ì •:
```python
cache_mgr = get_cache_manager(cache_limit_gb=30.0)  # ê¸°ë³¸ 50GB
```

### ë¬¸ì œ: ìºì‹œ ìƒì„± ì¤‘ë‹¨ë˜ì—ˆì„ ë•Œ

**í•´ê²°**:
- VLACacheManagerëŠ” ì´ë¯¸ ìƒì„±ëœ ìºì‹œë¥¼ ìë™ìœ¼ë¡œ ìŠ¤í‚µí•©ë‹ˆë‹¤
- ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì¤‘ë‹¨ëœ ë¶€ë¶„ë¶€í„° ê³„ì†ë©ë‹ˆë‹¤
- "skipped" ì¹´ìš´íŠ¸ë¡œ í™•ì¸ ê°€ëŠ¥

---

## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼

### ì´ì „ ì‹œìŠ¤í…œ
- âŒ Instruction ë³€ê²½ â†’ ìºì‹œ ë¯¸ìŠ¤
- âŒ Path ë³€ê²½ â†’ ìºì‹œ ë¯¸ìŠ¤
- âŒ ìºì‹œ ì ì¤‘ë¥ : ë¶ˆí™•ì‹¤

### ìƒˆ ì‹œìŠ¤í…œ
- âœ… Instruction ë³€ê²½ â†’ ìºì‹œ ìœ ì§€
- âœ… Path ë³€ê²½ â†’ ìºì‹œ ìœ ì§€
- âœ… ìºì‹œ ì ì¤‘ë¥ : ~100%

### í•™ìŠµ ì‹œì‘ ì‹œê°„ ê°œì„ 
- **ì´ì „**: ë§¤ë²ˆ VLM ì‹¤í–‰ (ëŠë¦¼)
- **ìƒˆë¡œìš´ ì‹œìŠ¤í…œ**: ìºì‹œ ë¡œë“œ (ë§¤ìš° ë¹ ë¦„)
- **ì˜ˆìƒ ê°œì„ **: 10-50ë°° ë¹ ë¥¸ ë°ì´í„° ë¡œë”©

---

## ğŸ‰ ê¶Œì¥ ë§ˆì´ê·¸ë ˆì´ì…˜ í”Œëœ

### Phase 1: í…ŒìŠ¤íŠ¸ (1ì‹œê°„)
1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ 1ê°œë¡œ ìºì‹œ ìƒì„±
2. Dataset ë¡œë”© í…ŒìŠ¤íŠ¸
3. TRAIN_Unified.pyë¡œ ì§§ì€ í•™ìŠµ í…ŒìŠ¤íŠ¸

### Phase 2: ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ (ì„ íƒ)
1. ì¤‘ìš”í•œ ë°ì´í„°ì…‹ë¶€í„° ìºì‹œ ìƒì„±
2. í•™ìŠµí•˜ë©´ì„œ ë‚˜ë¨¸ì§€ ìºì‹œ ìƒì„±

### Phase 3: ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ (ê¶Œì¥)
1. ê¸°ì¡´ ìºì‹œ ë°±ì—… ë˜ëŠ” ì‚­ì œ
2. ëª¨ë“  ë°ì´í„°ì…‹ ìºì‹œ ìƒì„± (Multi-GPU)
3. ì™„ë£Œ í›„ í•™ìŠµ ì‹œì‘

---

**ë§ˆì´ê·¸ë ˆì´ì…˜ ë‚ ì§œ**: 2025-11-03
**ì‘ì„±ì**: Claude Code
